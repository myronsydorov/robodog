{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03956540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2 \n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "import timm\n",
    "model_names = [\n",
    "    \"VGG-Face\", \"Facenet\", \"Facenet512\", \"OpenFace\", \"DeepFace\",\n",
    "    \"DeepID\", \"ArcFace\", \"Dlib\", \"SFace\", \"GhostFaceNet\",\n",
    "    \"Buffalo_L\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5deaf74-caf1-48cf-adc5-abc3894fd5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a60a9dfbd746e6b7a4f1698a74413e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/86.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_demo = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "\n",
    "model = timm.create_model(\n",
    "    'vit_small_patch16_224.dino',\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # remove classifier nn.Linear\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21fcda6e-25fd-4a01-b8ab-d6f3ab7ff8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 94,  79,  27, 299, 192]])\n"
     ]
    }
   ],
   "source": [
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "output = model(transforms(img_demo).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
    "\n",
    "top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n",
    "print(top5_class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40db2280-4c14-4f0e-948b-58fc1f1c5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b65e43-b082-41dd-824c-814ec8460563",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "# Remove the final classification layer → gives 512-d embedding\n",
    "embedding_model = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "embedding_model.eval()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Preprocessing transform for OpenCV images\n",
    "# --------------------------------------------------\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd97c918-2fa0-4551-80d8-ec8579645c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facial_verf(select_list, target, model_name):\n",
    "    target_list=[]\n",
    "    for i, susp in enumerate(select_list): \n",
    "        result = DeepFace.verify(target, susp, enforce_detection=False, model_name=model_name)\n",
    "        if result[\"verified\"]== True :\n",
    "            print(\"Target found\")\n",
    "            target_path = f\"target_found_{i}.jpg\"\n",
    "            target_list.append(susp)\n",
    "            cv2.imwrite(target_path, susp)\n",
    "    return target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b7f25d-dd9a-4e54-98c7-d09a0354cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_img_by_label(img, model, desired_label=\"person\"): \n",
    "    results = model(img)  # predict on an image\n",
    "    select_list=[]\n",
    "    # Get boxes and labels\n",
    "    boxes = results[0].boxes.xyxy  # Bounding box coordinates\n",
    "    labels = results[0].boxes.cls  # Class indices\n",
    "    class_names = model.names       # Mapping from class index to label name\n",
    "    \n",
    "    # Loop through detections\n",
    "    for i, (box, label_idx) in enumerate(zip(boxes, labels)):\n",
    "        label_name = class_names[int(label_idx)]\n",
    "        \n",
    "        # Only save boxes with the desired label\n",
    "        if label_name == desired_label:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            #print(f\" x1:{x1},x2:{x2},y1:{y1},y1:{y2}\")\n",
    "            cropped_img = img[y1:y2, x1:x2]\n",
    "            select_list.append(cropped_img)\n",
    "            cropped_image_path = f\"cropped_{desired_label}_{i}.jpg\"\n",
    "            #cv2.imwrite(cropped_image_path, cropped_img)\n",
    "            #print(f\"Saved: {cropped_image_path}\")\n",
    "    return select_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a829d29-eb4e-4b0b-9882-216415b4601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_target(target, model): \n",
    "    results = model(target)  # predict on an image\n",
    "    label_list=[]\n",
    "    # Get boxes and labels\n",
    "    boxes = results[0].boxes.xyxy  # Bounding box coordinates\n",
    "    labels = results[0].boxes.cls  # Class indices\n",
    "    class_names = model.names       # Mapping from class index to label name\n",
    "    \n",
    "    # Loop through detections\n",
    "    for i, (box, label_idx) in enumerate(zip(boxes, labels)):\n",
    "        label_name = class_names[int(label_idx)]\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cropped_img = target[y1:y2, x1:x2]\n",
    "        temp_dict= {\"label\":label_name,\"img\": cropped_img}\n",
    "        #print(temp_dict)\n",
    "        label_list.append(temp_dict)\n",
    "\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11400ee2-536b-44bd-9136-b1b68699048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return float(a @ b / ( (a**2).sum()**0.5 * (b**2).sum()**0.5 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a92f10f-6733-4005-88da-3e79947612b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(cv_img):\n",
    "    # BGR → RGB\n",
    "    img_rgb = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply transforms\n",
    "    tensor = transform(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        embedding = embedding_model(tensor)\n",
    "\n",
    "    # flatten from (1, 512, 1, 1) → (512,)\n",
    "    return embedding.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c469ee9-1cdf-4b2a-8e0f-776009260ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_label_sim(susp, target): \n",
    "\n",
    "# Get embeddings\n",
    "    emb1 = get_embedding(susp)\n",
    "    emb2 = get_embedding(target)\n",
    "\n",
    "# Similarity score\n",
    "    sim = cosine_similarity(emb1, emb2)\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dc30e92-0a10-43ce-ab90-e2b3dbc306fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_general_case(select_list, target, threshold):\n",
    "    target_list=[]\n",
    "    print(len(select_list))\n",
    "    for i, susp in enumerate(select_list): \n",
    "        sim = general_label_sim(susp, target)\n",
    "        print(sim)\n",
    "        if sim >= threshold:\n",
    "            print(\"Target found\")\n",
    "            target_path = f\"target_found_{i}.jpg\"\n",
    "            target_list.append(susp)\n",
    "            cv2.imwrite(target_path, susp)\n",
    "    return target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32b803bc-ff4b-40f2-9a4d-b41c8cb51b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load OpenCV images\n",
    "#img1 = cv2.imread(\"bag_1.jpg\")\n",
    "#img2 = cv2.imread(\"bag_3.jpg\")\n",
    "\n",
    "# Get embeddings\n",
    "#emb1 = get_embedding(img1)\n",
    "#emb2 = get_embedding(img2)\n",
    "\n",
    "# Similarity score\n",
    "#sim = cosine_similarity(emb1, emb2)\n",
    "\n",
    "#print(\"Cosine similarity:\", sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61a17d-a658-4b49-9cf2-2810760b2f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a88db3e-7776-4e13-8442-039fca468576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(img, target_img,threshold=0.8,model_for_detection=YOLO(\"yolo11n.pt\") , model_name_for_face_rec=\"SFace\"):\n",
    "    \"\"\" \n",
    "    img  cv_imread: \n",
    "        Image which is analyzed \n",
    "    target_img cv_imread: \n",
    "        Picture of object or person to be identified currently on called if label is person \n",
    "    label string \n",
    "        Name of a category which detection models categorizes \n",
    "    model_for_detection model that can be read by YOLO \n",
    "        Model that will categorize image by objects\n",
    "    model_name_for_face_rec model that can be read by DeepFace \n",
    "        Model that will do face verification if label is person\n",
    "    \"\"\"\n",
    "    meta_list=[]\n",
    "    #TODO: Verify that target image is of the object the label relates to \n",
    "    label_list= extract_label_target(target_img, model_for_detection)\n",
    "    for label_dict in label_list: \n",
    "        label_name= label_dict[\"label\"]\n",
    "        print(f\"label is {label_name}\")\n",
    "        label_target= label_dict[\"img\"]\n",
    "        select_list = select_img_by_label(img, model_for_detection, desired_label=label_name)\n",
    "        if label_name==\"person\":\n",
    "            #print(\"how got i here\")\n",
    "            target_list=facial_verf(select_list, target_img, model_name_for_face_rec)\n",
    "            meta_list.append(target_list)\n",
    "        else:\n",
    "            #print(\"i got here at least\")\n",
    "            target_list=list_general_case(select_list, target_img, threshold)\n",
    "            meta_list.append(target_list)\n",
    "    return meta_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a3039c1-51ea-42a5-8f9f-d924fd5e1d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 8 persons, 1 cup, 2 laptops, 1 cell phone, 86.2ms\n",
      "Speed: 1.6ms preprocess, 86.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "label is laptop\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 112.9ms\n",
      "Speed: 3.1ms preprocess, 112.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "0\n",
      "label is cup\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 55.2ms\n",
      "Speed: 2.8ms preprocess, 55.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "1\n",
      "0.4523012340068817\n",
      "label is person\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 39.6ms\n",
      "Speed: 1.8ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "label is laptop\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 50.6ms\n",
      "Speed: 2.1ms preprocess, 50.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "0\n",
      "label is person\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 70.3ms\n",
      "Speed: 2.7ms preprocess, 70.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "label is person\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 46.6ms\n",
      "Speed: 1.7ms preprocess, 46.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "label is cell phone\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 45.0ms\n",
      "Speed: 1.8ms preprocess, 45.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "0\n",
      "label is person\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 86.4ms\n",
      "Speed: 1.7ms preprocess, 86.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "label is person\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 43.8ms\n",
      "Speed: 1.8ms preprocess, 43.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "label is person\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 45.0ms\n",
      "Speed: 1.8ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "label is person\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 39.0ms\n",
      "Speed: 1.7ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "label is person\n",
      "\n",
      "0: 640x480 1 person, 1 cup, 1 dining table, 40.6ms\n",
      "Speed: 1.7ms preprocess, 40.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], [], [], [], [], [], [], [], []]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label= \"person\"\n",
    "img1 = cv2.imread(\"cup_close2.jpg\")\n",
    "img2 = cv2.imread(\"cup_scene.jpg\")\n",
    "analyze_image(img1, img2, 0.8, model, model_names[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37d541-b69b-4b88-ab68-03db8bad3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144d308-14b4-48e7-bab5-b9a225562e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_img= cv2.imread(\"top_gear.webp\")  \n",
    "results = model(pred_img)  # predict on an image\n",
    "\n",
    "desired_label = \"person\"  # Replace with the label you want\n",
    "people_list=[]\n",
    "# Get boxes and labels\n",
    "boxes = results[0].boxes.xyxy  # Bounding box coordinates\n",
    "labels = results[0].boxes.cls  # Class indices\n",
    "class_names = model.names       # Mapping from class index to label name\n",
    "\n",
    "# Loop through detections\n",
    "for i, (box, label_idx) in enumerate(zip(boxes, labels)):\n",
    "    label_name = class_names[int(label_idx)]\n",
    "    \n",
    "    # Only save boxes with the desired label\n",
    "    if label_name == desired_label:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        print(f\" x1:{x1},x2:{x2},y1:{y1},y1:{y2}\")\n",
    "        cropped_img = pred_img[y1:y2, x1:x2]\n",
    "        people_list.append(cropped_img)\n",
    "        cropped_image_path = f\"cropped_{desired_label}_{i}.jpg\"\n",
    "        cv2.imwrite(cropped_image_path, cropped_img)\n",
    "        print(f\"Saved: {cropped_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1ca58-1619-4d95-bbf4-cd5e8c48a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img= cv2.imread(\"matt_le_blanc.jpg\") \n",
    "print(models[3])\n",
    "for i, susp in enumerate(people_list): \n",
    "    result = DeepFace.verify(target_img, susp, enforce_detection=False, model_name=models[8])\n",
    "    if result[\"verified\"]== True :\n",
    "        print(\"Target found\")\n",
    "        target_path = f\"target_found_{i}.jpg\"\n",
    "        cv2.imwrite(target_path, susp)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3b339e6-0019-4c33-9155-b90aa1f14c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/alex/Documents/EDTH2025/robot_part/cup_close.jpg: 640x480 1 cup, 1 spoon, 2 keyboards, 82.9ms\n",
      "Speed: 1.6ms preprocess, 82.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "results = model(\"cup_close.jpg\")  # results list\n",
    "\n",
    "# Visualize the results\n",
    "for i, r in enumerate(results):\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot()  # BGR-order numpy array\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "\n",
    "    # Show results to screen (in supported environments)\n",
    "    r.show()\n",
    "\n",
    "    # Save results to disk\n",
    "    r.save(filename=f\"cup_close{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c541a-51cd-4745-b3f3-8d9dd6473eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Hack_3.11)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
